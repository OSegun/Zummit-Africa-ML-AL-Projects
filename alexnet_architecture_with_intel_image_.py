# -*- coding: utf-8 -*-
"""Alexnet Architecture with Intel Image .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_VU3Zm7FrJy0hRGnyZ6_RWS40Dub9j0z
"""

# Importing all dependencies
import numpy as np
from keras import layers
from keras.layers import Input, Dense, Activation,BatchNormalization, Flatten, Conv2D, MaxPooling2D, Dropout
from keras.models import Model 
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
import keras.backend as K
K.set_image_data_format('channels_last')
import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow
import os

def AlexNet(input_shape): # Building the Alexnet Nural Network
    
  X_input = Input(input_shape) # Shape of the Image

  # 1st Convolutional Layer  
  X = Conv2D(96,(11,11),strides = 4,name="conv0")(X_input)
  # Batch Normalisation before passing it to the next layer
  X = BatchNormalization(axis = 3 , name = "bn0")(X)
  X = Activation('relu')(X)
  
  # Pooling   
  X = MaxPooling2D((3,3),strides = 2,name = 'max0')(X)
  
  # 2nd Convolutional Layer
  X = Conv2D(256,(5,5),padding = 'same' , name = 'conv1')(X)
  # Batch Normalisation before passing it to the next layer
  X = BatchNormalization(axis = 3 ,name='bn1')(X)
  X = Activation('relu')(X)

  # Pooling  
  X = MaxPooling2D((3,3),strides = 2,name = 'max1')(X)
    
  # 3rd Convolutional Layer
  X = Conv2D(384, (3,3) , padding = 'same' , name='conv2')(X)
  # Batch Normalisation before passing it to the next layer
  X = BatchNormalization(axis = 3, name = 'bn2')(X)
  X = Activation('relu')(X)
  
  # 4th Convolutional Layer
  X = Conv2D(384, (3,3) , padding = 'same' , name='conv3')(X)
  # Batch Normalisation before passing it to the next layer
  X = BatchNormalization(axis = 3, name = 'bn3')(X)
  X = Activation('relu')(X)
  
  # 5th Convolutional Layer
  X = Conv2D(256, (3,3) , padding = 'same' , name='conv4')(X)
  # Batch Normalisation before passing it to the next layer
  X = BatchNormalization(axis = 3, name = 'bn4')(X)
  X = Activation('relu')(X)
    
  # Pooling
  X = MaxPooling2D((3,3),strides = 2,name = 'max2')(X)

  # Passing it to a dense layer  
  X = Flatten()(X)
  
  # 1st Dense Layer
  X = Dense(4096, activation = 'relu', name = "fc0")(X)
  X = Dropout(0.4)(X)
  # Batch Normalisation
  X = BatchNormalization()(X)

  # 2nd Dense Layer  
  X = Dense(4096, activation = 'relu', name = 'fc1')(X)
  X = Dropout(0.4)(X)
  # Batch Normalisation
  X = BatchNormalization()(X)
    
  X = Dense(6,activation='softmax',name = 'fc2')(X)
    
  model = Model(inputs = X_input, outputs = X, name='AlexNet')
  return model

"""The Intel Image use case dataset contains around 25k images of size 150x150 distributed under 6 categories, namely : ‘buildings’ , ‘forest’ , ‘glacier’ , ‘mountain’ , ‘sea’ , ‘street’ . There are 14K images in training set, 3K in test setand 7K in Prediction set.

The data images for all the categories are split into it’s respective directories train, test and pred respectively, thus making it easy to infer the labels

The dataset can be assessed via kaggle website : https://www.kaggle.com/datasets/puneet6060/intel-image-classification
"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json # Download the json file from your kaggle profile to use the kaggle api in your notebook

!kaggle datasets download -d puneet6060/intel-image-classification

dataset = '/content/intel-image-classification.zip'
from zipfile import ZipFile
with ZipFile(dataset, 'r') as zip:
  zip.extractall()
  print('The dataset is extracted')

#import os
#import shutil

#os.listdir("/content/seg_train") #First find where the ".ipynb_checkpoints" is located.

#shutil.rmtree("directory_where_.ipynb_checkpoints_is_located/.ipynb_checkpoints") #be careful with shutil.rmtree() because it deletes every tree in that path. In other words, do not make mistakes.

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

train = train_datagen.flow_from_directory('/content/seg_train/seg_train', target_size=(227,227),
                                          shuffle = True,
                                          class_mode='categorical')

print("Batch Size for Input Image : ",train[0][0].shape)
print("Batch Size for Output Image : ",train[0][1].shape)
print("Image Size of first image : ",train[0][0][0].shape)
print("Output of first image : ",train[0][1][0].shape)

test_datagen = ImageDataGenerator(rescale=1. / 255)
test = test_datagen.flow_from_directory('/content/seg_test/seg_test', target_size=(227,227), class_mode='categorical')

predict_datagen = ImageDataGenerator(rescale=1. / 255)
predict = predict_datagen.flow_from_directory('/content/seg_pred', target_size=(227,227), batch_size = 1,class_mode='categorical')

fig , axs = plt.subplots(2,3 ,figsize = (10,10))
axs[0][0].imshow(train[0][0][12])
axs[0][0].set_title(train[0][1][12])
axs[0][1].imshow(train[0][0][10])
axs[0][1].set_title(train[0][1][10])
axs[0][2].imshow(train[0][0][5])
axs[0][2].set_title(train[0][1][5])
axs[1][0].imshow(train[0][0][20])
axs[1][0].set_title(train[0][1][20])
axs[1][1].imshow(train[0][0][25])
axs[1][1].set_title(train[0][1][25])
axs[1][2].imshow(train[0][0][3])
axs[1][2].set_title(train[0][1][3])

alex = AlexNet(train[0][0].shape[1:]) # Using the Alexnet Neural Network on the train dataset

alex.summary()

alex.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics=['accuracy'])
alex.fit(train,epochs=50)

preds = alex.evaluate_generator(test)
print ("Loss = " + str(preds[0]))
print ("Test Accuracy = " + str(preds[1]))

predictions = alex.predict_generator(predict)

print(predictions[700])

print(np.argmax(predictions[700]))

print(os.listdir('/content/seg_train/seg_train')[np.argmax(predictions[700])])

for i in range(7301):
  print(os.listdir('/content/seg_train/seg_train')[np.argmax(predictions[i])])

